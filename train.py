# -*- coding: utf-8 -*-
"""Copy of EVA4 - Session 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOUIjtw8CvnmMqT494jZI3nYm2QOipz7
"""

from __future__ import print_function

import os

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Subset
from torchsummary import summary
from torchvision import datasets, transforms
from tqdm import tqdm  # Add this import

# Comment out logging setup
# os.makedirs("logs", exist_ok=True)
# timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - %(message)s",
#     handlers=[
#         logging.FileHandler(f"logs/training_{timestamp}.txt"),
#         logging.StreamHandler(sys.stdout),
#     ],
# )
# logger = logging.getLogger(__name__)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # First block - reduced initial channels
        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(10)
        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(20)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout2d(0.05)  # Reduced dropout

        # Second block - reduced channels
        self.conv3 = nn.Conv2d(20, 30, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(30)
        self.conv4 = nn.Conv2d(30, 40, 3, padding=1)
        self.bn4 = nn.BatchNorm2d(40)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.dropout2 = nn.Dropout2d(0.05)

        # Final block - now using GAP
        self.conv5 = nn.Conv2d(40, 10, 1)  # 1x1 convolution to get 10 channels
        self.gap = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling layer

    def forward(self, x):
        # First block
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool1(x)
        x = self.dropout1(x)

        # Second block
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool2(x)
        x = self.dropout2(x)

        # Final block with GAP
        x = self.conv5(x)
        x = self.gap(x)
        x = x.view(-1, 10)  # Safe to use -1 here as GAP ensures fixed dimensions
        return F.log_softmax(x, dim=1)


def train(model, device, train_loader, optimizer):
    model.train()
    pbar = tqdm(train_loader)
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        pbar.set_postfix(loss=f"{loss.item():.4f}", batch=batch_idx)


def test(model, device, test_loader, save_misclassified=False, epoch=None):
    model.eval()
    test_loss = 0
    correct = 0
    misclassified_images = []

    # Create directory for misclassified images if it doesn't exist
    if save_misclassified and epoch is not None:
        save_dir = f"misclassified_images_{epoch}"
        os.makedirs(save_dir, exist_ok=True)

    with torch.no_grad():
        pbar = tqdm(test_loader)
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction="sum").item()
            pred = output.argmax(dim=1, keepdim=True)

            # Find misclassified images
            incorrect_mask = ~pred.eq(target.view_as(pred)).squeeze()
            if save_misclassified and epoch is not None and incorrect_mask.any():
                misclassified_data = data[incorrect_mask]
                misclassified_targets = target[incorrect_mask]
                misclassified_preds = pred[incorrect_mask]

                # Save misclassified images
                for idx, (img, true_label, pred_label) in enumerate(
                    zip(misclassified_data, misclassified_targets, misclassified_preds)
                ):
                    img_np = img.cpu().numpy().squeeze()
                    plt.figure(figsize=(3, 3))
                    plt.imshow(img_np, cmap="gray")
                    plt.title(f"True: {true_label.item()}, Pred: {pred_label.item()}")
                    plt.axis("off")
                    plt.savefig(
                        os.path.join(
                            save_dir,
                            f"misclassified_{batch_idx}_{idx}_true{true_label.item()}_pred{pred_label.item()}.png",
                        )
                    )
                    plt.close()

            correct += pred.eq(target.view_as(pred)).sum().item()
            current_loss = test_loss / ((batch_idx + 1) * len(data))
            current_acc = 100.0 * correct / ((batch_idx + 1) * len(data))
            pbar.set_postfix(loss=f"{current_loss:.4f}", acc=f"{current_acc:.2f}%")

    test_loss /= len(test_loader.dataset)
    accuracy = 100.0 * correct / len(test_loader.dataset)

    print(
        f"\nEpoch:{epoch} Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\n"
    )
    # logger.info(log_msg)

    return accuracy


def get_train_val_loaders(batch_size, use_cuda, train_size=None):
    kwargs = {"num_workers": 1, "pin_memory": True} if use_cuda else {}

    # Enhanced transform with augmentation for training
    train_transform = transforms.Compose(
        [
            # Reduced rotation to 7 degrees max
            transforms.RandomRotation((-7, 7), fill=0),
            transforms.RandomAffine(
                degrees=0,
                translate=(0.1, 0.1),  # Slight translation
                scale=(0.95, 1.05),  # More conservative scale
                shear=(-7, 7),  # Reduced shear
                fill=0,
            ),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,)),
            transforms.RandomErasing(p=0.1, scale=(0.02, 0.08)),
        ]
    )

    # Regular transform for validation (no augmentation)
    val_transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
    )

    # Get datasets with different transforms
    full_train_dataset = datasets.MNIST(
        "../data", train=True, download=True, transform=train_transform
    )

    full_val_dataset = datasets.MNIST(
        "../data", train=True, download=True, transform=val_transform
    )

    # Calculate split sizes
    total_size = len(full_train_dataset)
    if train_size is None:  # If train_size not specified, use 80% of data
        train_size = int(0.8 * total_size)
    val_size = total_size - train_size

    # Create indices for the split
    indices = list(range(total_size))
    np.random.shuffle(indices)
    train_indices = indices[:train_size]
    val_indices = indices[train_size : train_size + val_size]

    # Create train and validation datasets
    train_dataset = Subset(full_train_dataset, train_indices)
    val_dataset = Subset(full_val_dataset, val_indices)

    # Create data loaders
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, **kwargs
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, **kwargs
    )

    return train_loader, val_loader


def main():
    print("Starting training with configuration:")
    print(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print(f"Batch size: {batch_size}")
    print(f"Epochs: {epochs}")
    print("Model architecture:")

    # Set random seed for reproducibility
    torch.manual_seed(1)

    # Check CUDA availability
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # Training hyperparameters
    batch_size = 128
    epochs = 20

    train_loader, val_loader = get_train_val_loaders(batch_size, use_cuda)
    model = Net().to(device)

    # Print model summary
    summary(model, input_size=(1, 28, 28))

    # Initialize optimizer with Adam instead of SGD
    optimizer = optim.Adam(
        model.parameters(),
        lr=0.003,  # Standard Adam learning rate
        betas=(0.9, 0.999),  # Default Adam betas
        eps=1e-08,  # Default epsilon
        weight_decay=1e-4,
    )

    # Modified learning rate scheduler for Adam
    scheduler = optim.lr_scheduler.StepLR(
        optimizer,
        step_size=4,  # Increased step size for Adam
        gamma=0.6,  # Less aggressive decay for Adam
    )

    # Training loop
    best_accuracy = 0
    for epoch in range(1, epochs + 1):
        print(f"\nEpoch {epoch}/{epochs}")
        print(f"Learning rate: {scheduler.get_last_lr()[0]}")

        train(model, device, train_loader, optimizer)
        accuracy = test(model, device, val_loader, save_misclassified=True, epoch=epoch)
        scheduler.step()

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), "best_model.pth")
            print(f"New best accuracy: {accuracy:.2f}%")

        print(f"Epoch: {epoch}, Current LR: {scheduler.get_last_lr()[0]}")


if __name__ == "__main__":
    main()
