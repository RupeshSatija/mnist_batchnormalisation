# -*- coding: utf-8 -*-
"""Copy of EVA4 - Session 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOUIjtw8CvnmMqT494jZI3nYm2QOipz7
"""

from __future__ import print_function

import logging
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Subset
from torchsummary import summary
from torchvision import datasets, transforms
from tqdm import tqdm  # Add this import

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("training_logs.txt"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # First block - reduced initial channels
        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(10)
        self.conv2 = nn.Conv2d(10, 16, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout2d(0.15)  # Reduced dropout

        # Second block - reduced channels
        self.conv3 = nn.Conv2d(16, 24, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(24)
        self.conv4 = nn.Conv2d(24, 32, 3, padding=1)
        self.bn4 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.dropout2 = nn.Dropout2d(0.15)

        # Final block - now using GAP
        self.conv5 = nn.Conv2d(32, 10, 1)  # 1x1 convolution to get 10 channels
        self.gap = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling layer

    def forward(self, x):
        # First block
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool1(x)
        x = self.dropout1(x)

        # Second block
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool2(x)
        x = self.dropout2(x)

        # Final block with GAP
        x = self.conv5(x)
        x = self.gap(x)
        x = x.view(-1, 10)  # Safe to use -1 here as GAP ensures fixed dimensions
        return F.log_softmax(x, dim=1)


def train(model, device, train_loader, optimizer):
    model.train()
    pbar = tqdm(train_loader)
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        desc = f"loss={loss.item():.4f} batch_id={batch_idx}"
        pbar.set_description(desc)
        logger.info(f"Training - Batch: {batch_idx}, Loss: {loss.item():.4f}")


def test(model, device, test_loader, save_misclassified=False, epoch=None):
    model.eval()
    test_loss = 0
    correct = 0
    misclassified_images = []

    # Create directory for misclassified images if it doesn't exist
    if save_misclassified and epoch is not None:
        save_dir = f"misclassified_images_{epoch}"
        os.makedirs(save_dir, exist_ok=True)

    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction="sum").item()
            pred = output.argmax(dim=1, keepdim=True)

            # Find misclassified images
            incorrect_mask = ~pred.eq(target.view_as(pred)).squeeze()
            if save_misclassified and epoch is not None and incorrect_mask.any():
                misclassified_data = data[incorrect_mask]
                misclassified_targets = target[incorrect_mask]
                misclassified_preds = pred[incorrect_mask]

                # Save misclassified images
                for idx, (img, true_label, pred_label) in enumerate(
                    zip(misclassified_data, misclassified_targets, misclassified_preds)
                ):
                    img_np = img.cpu().numpy().squeeze()
                    plt.figure(figsize=(3, 3))
                    plt.imshow(img_np, cmap="gray")
                    plt.title(f"True: {true_label.item()}, Pred: {pred_label.item()}")
                    plt.axis("off")
                    plt.savefig(
                        os.path.join(
                            save_dir,
                            f"misclassified_{batch_idx}_{idx}_true{true_label.item()}_pred{pred_label.item()}.png",
                        )
                    )
                    plt.close()

            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    accuracy = 100.0 * correct / len(test_loader.dataset)

    log_msg = f"\nEpoch {epoch} Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\n"
    logger.info(log_msg)
    print(log_msg)

    return accuracy


def get_train_val_loaders(batch_size, use_cuda, train_size=None):
    kwargs = {"num_workers": 1, "pin_memory": True} if use_cuda else {}

    # Enhanced transform with augmentation for training
    train_transform = transforms.Compose(
        [
            # Reduced rotation to 7 degrees max
            transforms.RandomRotation((-7.0, 7.0), fill=0),
            transforms.RandomAffine(
                degrees=0,
                translate=(0.1, 0.1),  # Slight translation
                scale=(0.9, 1.1),  # More conservative scale
                shear=(-10, 10),  # Reduced shear
                fill=0,
            ),
            transforms.RandomPerspective(distortion_scale=0.2, p=0.5, fill=0),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,)),
            transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),
        ]
    )

    # Regular transform for validation (no augmentation)
    val_transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
    )

    # Get datasets with different transforms
    full_train_dataset = datasets.MNIST(
        "../data", train=True, download=True, transform=train_transform
    )

    full_val_dataset = datasets.MNIST(
        "../data", train=True, download=True, transform=val_transform
    )

    # Calculate split sizes
    total_size = len(full_train_dataset)
    if train_size is None:  # If train_size not specified, use 80% of data
        train_size = int(0.8 * total_size)
    val_size = total_size - train_size

    # Create indices for the split
    indices = list(range(total_size))
    np.random.shuffle(indices)
    train_indices = indices[:train_size]
    val_indices = indices[train_size : train_size + val_size]

    # Create train and validation datasets
    train_dataset = Subset(full_train_dataset, train_indices)
    val_dataset = Subset(full_val_dataset, val_indices)

    # Create data loaders
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, **kwargs
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, **kwargs
    )

    return train_loader, val_loader


def main():
    # Set random seed for reproducibility
    torch.manual_seed(1)

    # Check CUDA availability
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # Training hyperparameters
    batch_size = 64
    epochs = 20

    train_loader, val_loader = get_train_val_loaders(batch_size, use_cuda)
    model = Net().to(device)

    # Print model summary
    summary(model, input_size=(1, 28, 28))

    # Initialize optimizer with Adam instead of SGD
    optimizer = optim.Adam(
        model.parameters(),
        lr=0.001,  # Standard Adam learning rate
        betas=(0.9, 0.999),  # Default Adam betas
        eps=1e-08,  # Default epsilon
        weight_decay=1e-4,  # L2 regularization
    )

    # Modified learning rate scheduler for Adam
    scheduler = optim.lr_scheduler.StepLR(
        optimizer,
        step_size=5,  # Increased step size for Adam
        gamma=0.5,  # Less aggressive decay for Adam
    )

    # Training loop
    best_accuracy = 0
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer)
        accuracy = test(model, device, val_loader, save_misclassified=True, epoch=epoch)
        scheduler.step()

        # Save best model
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), "best_model.pth")

        # Print current learning rate
        print(f"Epoch: {epoch}, Current LR: {scheduler.get_last_lr()[0]}")


if __name__ == "__main__":
    main()
